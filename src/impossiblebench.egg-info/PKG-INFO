Metadata-Version: 2.4
Name: impossiblebench
Version: 0.1.0
Summary: A benchmark framework for measuring LLM agents' propensity to exploit test cases
Home-page: https://github.com/fjzzq2002/impossiblebench
Author: ImpossibleBench Team
Keywords: llm benchmark evaluation testing cheating swe-bench livecodebench humaneval
Classifier: Development Status :: 3 - Alpha
Classifier: Intended Audience :: Science/Research
Classifier: Topic :: Scientific/Engineering :: Artificial Intelligence
Classifier: Programming Language :: Python :: 3
Classifier: Programming Language :: Python :: 3.10
Classifier: Programming Language :: Python :: 3.11
Classifier: Programming Language :: Python :: 3.12
Requires-Python: >=3.10
Description-Content-Type: text/markdown
License-File: LICENSE
Requires-Dist: inspect_ai>=0.3.0
Requires-Dist: inspect_evals[swe_bench]@ git+https://github.com/UKGovernmentBEIS/inspect_evals
Requires-Dist: pandas>=2.0.0
Requires-Dist: datasets>=3.0.0
Requires-Dist: tqdm>=4.60.0
Requires-Dist: jsonlines>=4.0.0
Requires-Dist: swebench>=4.0.0
Requires-Dist: platformdirs>=4.0.0
Provides-Extra: analysis
Requires-Dist: anthropic>=0.40.0; extra == "analysis"
Requires-Dist: litellm>=1.50.0; extra == "analysis"
Provides-Extra: all
Requires-Dist: anthropic>=0.40.0; extra == "all"
Requires-Dist: litellm>=1.50.0; extra == "all"
Dynamic: author
Dynamic: classifier
Dynamic: description
Dynamic: description-content-type
Dynamic: home-page
Dynamic: keywords
Dynamic: license-file
Dynamic: provides-extra
Dynamic: requires-dist
Dynamic: requires-python
Dynamic: summary

# ImpossibleBench

[ImpossibleBench](https://arxiv.org/abs/2510.20270) is a benchmark framework that systematically measures LLM agents' propensity to exploit test cases by creating "impossible" variants of tasks where passing necessarily implies specification-violating shortcuts or "cheating."

![Illustration of the paper](fig1.png)

This repository provides the official Inspect AI implementation for ImpossibleBench evaluations. The benchmark datasets are available on HuggingFace for use with any other evaluation framework:

- [Impossible-LiveCodeBench](https://huggingface.co/datasets/fjzzq2002/impossible_livecodebench)
- [Impossible-SWEbench](https://huggingface.co/datasets/fjzzq2002/impossible_swebench)

## Installation

Install the package directly from source:

```bash
# Clone the repository
git clone https://github.com/safety-research/impossiblebench
cd impossiblebench
pip install -e .
```

For SWE-bench evaluation, Docker is required for sandboxed execution. Docker is optional but recommended for LiveCodeBench evaluation.

## Usage

### Quick Start

```python
from inspect_ai import eval
from impossiblebench import impossible_livecodebench, impossible_swebench

# LiveCodeBench evaluation with minimal scaffold
task = impossible_livecodebench(
    split="conflicting",   # "original", "oneoff", or "conflicting"
    agent_type="minimal",  # Simple submission loop
    limit=10,              # Run on first 10 samples
)

eval(task, model="openai/gpt-4o")
```

```python
# SWE-bench evaluation with full tool-based scaffold
task = impossible_swebench(
    split="conflicting",   # "original", "oneoff", or "conflicting"
    agent_type="tools",    # Full scaffold with bash, python, text editor
    limit=5,               # Run on first 5 samples (slower)
)

eval(task, model="anthropic/claude-3-5-sonnet-20241022")
```

### Replicating Paper Results

To replicate the exact experimental settings from the paper:

```bash
# Replicate LiveCodeBench experiments (all agent types and splits)
python demo.py replicate_lcb

# Replicate SWE-bench experiments (all agent types and splits)
python demo.py replicate_swe
```

These commands use the exact configuration from our paper experiments, including:
- The default tuned prompt
- Both agent types (minimal and tools)
- All splits (oneoff, conflicting, and open-test original)
- Appropriate limits

By default, these replication scripts run on a small subset of the data with a single model for demonstration purposes. See instructions in the demo.py file to run the full experiments. We also provided some additional demo examples in the demo.py file.

**Note**: We recommend using the Python API rather than the Inspect CLI for this benchmark.

## Agent Scaffolds

ImpossibleBench currently implements two different scaffold configurations for each of the tasks:

### Minimal Scaffold
- Simple submission loop without tools
- Recommended for: LiveCodeBench (single-file tasks)

### Full Scaffold
- Complex scaffold with multiple tools (bash, python, text_editor, think)
- Agent interacts with sandbox via tool calls
- Recommended for: SWE-bench (multi-file tasks)

## Analysis

We provide comprehensive analysis tools in the `analysis/` directory for processing evaluation results.

### Data Loading and Analysis

After running evaluations to generate logs, use the `DataLoader` class to load and analyze the results:

```python
from impossiblebench.analysis import DataLoader

# Load evaluation results
loader = DataLoader(n_workers=4)
loader.load_folder("/path/to/logs")

# Convert to DataFrame
df = loader.to_df()

# Get summary statistics
summary = loader.get_summary()
print(summary)
```

### LLM Judge Analysis

The `LLMJudge` class enables automated classification of agent behavior:

```python
from impossiblebench.analysis import LLMJudge

# Create judge instance
judge = LLMJudge(model="claude-opus-4-20250514")

# Binary classification (cheating vs legit)
binary_results = await judge.batch_evaluate_binary(transcript_dumps)

# Type classification (A-E categories)
type_results = await judge.batch_evaluate_type(transcript_dumps)
```

### Example Usage Script

See `analysis/example_usage.py` for a complete demonstration of the analysis tools.

```bash
# Basic usage
python analysis/example_usage.py /path/to/logs

# Multiple log folders
python analysis/example_usage.py /path/to/logs1 /path/to/logs2 --output results.csv

# Skip LLM analysis
python analysis/example_usage.py /path/to/logs --llm-samples 0
```

## Citation

If you use ImpossibleBench in your research, please cite:

```bibtex
@misc{zhong2025impossiblebench,
  title         = {ImpossibleBench: Measuring LLMs' Propensity of Exploiting Test Cases},
  author        = {Zhong, Ziqian and Raghunathan, Aditi and Carlini, Nicholas},
  year          = {2025},
  eprint        = {2510.20270},
  archivePrefix = {arXiv},
  primaryClass  = {cs.LG},
  doi           = {10.48550/arXiv.2510.20270},
  url           = {https://arxiv.org/abs/2510.20270}
}
```
